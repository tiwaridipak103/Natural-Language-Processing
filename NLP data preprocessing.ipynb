{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcfd8295-98d9-404f-a6b3-6ef579d00aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f8cd98f-205a-4adf-95dd-0b9f9abf6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')  # Compile regex for HTML tags\n",
    "    cleantext = re.sub(cleanr, '', raw_html)  # Remove HTML tags\n",
    "    result = re.sub(r\"http\\S+\", \"\", cleantext)  # Remove URLs\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eebdf064-86d8-403e-8515-1232694cf2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def chunk(string):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(string)))  # Perform POS tagging and NER\n",
    "    current_chunk = []\n",
    "    \n",
    "    for i in chunked:\n",
    "        if type(i) == Tree and i.label() == 'PERSON':  # Check if the chunk is a PERSON entity\n",
    "            # Join tokens of the entity and filter by length\n",
    "            complete = \"_\".join([token for token, pos in i.leaves() if len(token) > 2])\n",
    "            current_chunk.append(complete)\n",
    "        else:\n",
    "            current_chunk.append(i[0])\n",
    "    \n",
    "    return ' '.join(current_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14bf4a90-21b6-413a-9548-8f233990a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_underscore(string):\n",
    "    e = []\n",
    "    \n",
    "    for i in string.split(' '):\n",
    "        if len(i) >= 1:\n",
    "            if i[0] == '_' and i[-1] == '_':  # Check for leading and trailing underscores\n",
    "                e.append(i[1:-1])\n",
    "            elif i[0] == '_':  # Check for leading underscore\n",
    "                e.append(i[1:])\n",
    "            elif i[-1] == '_':  # Check for trailing underscore\n",
    "                e.append(i[:-1])\n",
    "            else:\n",
    "                e.append(i)\n",
    "    \n",
    "    return ' '.join(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f30df33e-20a0-47fd-aa86-3d36928f31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # Specific contractions\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
    "\n",
    "    # General contractions\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "\n",
    "    return phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cee7c851-0f2a-4b99-9a02-366eb7dd6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(Input_Text):\n",
    "    preprocessed_string = []\n",
    "    \n",
    "    for text in Input_Text:\n",
    "        # Remove specific patterns and replace with spaces\n",
    "        x = cleanhtml(text).replace(\"from:\", \"\").replace(\"write to:\", \"\").replace('\\\\n',' ').replace('\\\\t',' ')\n",
    "        # Remove all the special characters: except space ' '\n",
    "        remove_char_word = lambda y: ' '.join(j for j in y.split() if ':' not in j)\n",
    "        x = remove_char_word(x)\n",
    "        # remove anything between the square brackets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a27b47-b13c-414e-9f7d-189ef9d787cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_ita(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = decontractions(text)  # Expand contractions\n",
    "    text = re.sub('[$)\\?\"’.°!;\\'€%:,(/]', '', text)  # Remove specific characters\n",
    "    text = re.sub('\\u200b', ' ', text)  # Replace special characters with space\n",
    "    text = re.sub('\\xa0', ' ', text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a319f1-4965-4435-857d-023b82f4cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = decontractions(text)  # Expand contractions\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', '', text)  # Remove special characters except space\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a55152-083a-4e44-927b-fd75a06e636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Dipak/NNP)\n",
      "  kumar/NN\n",
      "  is/VBZ\n",
      "  go/JJ\n",
      "  (PERSON San/NNP Francisco/NNP))\n",
      "Original Sentence: Dipak kumar is go San Francisco\n",
      "Corrected Sentence: Dipak kumar is go San Francisco\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Download NLTK resources (uncomment the line below if you haven't downloaded them before)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def correct_sentence(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    # Perform POS tagging\n",
    "    tagged_words = pos_tag(words)\n",
    "    print(ne_chunk(tagged_words))\n",
    "\n",
    "    # Initialize an empty list for corrected words\n",
    "    corrected_words = []\n",
    "\n",
    "    # Correct the sentence based on POS tags\n",
    "    for word, tag in tagged_words:\n",
    "        if tag.startswith('VB'):  # Check if the tag indicates a verb\n",
    "            # Replace 'go' with 'going' for present continuous tense\n",
    "            if word == 'go':\n",
    "                corrected_words.append('going')\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "\n",
    "    # Join the corrected words to form the final corrected sentence\n",
    "    corrected_sentence = ' '.join(corrected_words)\n",
    "\n",
    "    return corrected_sentence\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Dipak kumar is go San Francisco\"\n",
    "corrected_sentence = correct_sentence(sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f370546e-2120-472c-badf-4762c82af823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Dipak/NNP)\n",
      "  kumar/NN\n",
      "  is/VBZ\n",
      "  going/VBG\n",
      "  (PERSON San/NNP Francisco/NNP))\n",
      "Named Entities: ['Dipak', 'San Francisco']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# Sentence to be analyzed\n",
    "sentence = \"Dipak kumar is going San Francisco\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "# Perform NER using ne_chunk\n",
    "ne_tagged = ne_chunk(tagged_words)\n",
    "\n",
    "print(ne_tagged)\n",
    "\n",
    "# Initialize an empty list for capturing named entities\n",
    "named_entities = []\n",
    "\n",
    "# Traverse the named entity tree to capture named entities\n",
    "for subtree in ne_tagged:\n",
    "    if isinstance(subtree, nltk.Tree):  # If subtree is a named entity\n",
    "        entity = \" \".join([token for token, tag in subtree.leaves()])\n",
    "        named_entities.append(entity)\n",
    "\n",
    "print(\"Named Entities:\", named_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8c283-4d65-410b-b4c1-5ded266ccf5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
