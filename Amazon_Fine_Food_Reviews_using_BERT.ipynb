{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5el_8SqFqVAT"
      },
      "source": [
        "\n",
        "In this notebook, You will do amazon review classification with BERT.[Download data from [this](https://www.kaggle.com/snap/amazon-fine-food-reviews/data) link]\n",
        "<pre>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOtG4cf0qVAZ"
      },
      "source": [
        "#all imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcmiHdAJqVAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cca64024-cf0b-4443-9f73-2927f19d941c"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTWRqbrBqVAu"
      },
      "source": [
        "<pre><font size=6>Part-1: Preprocessing</font></pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3csZKDrqVAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91df7cd2-08a9-47e7-d0c5-0c9d96cc426c"
      },
      "source": [
        "#Read the dataset - Amazon fine food reviews\n",
        "reviews = pd.read_csv(\"Reviews.csv\")\n",
        "#reviews.to_frame()\n",
        "#check the info of the dataset\n",
        "reviews.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 568454 entries, 0 to 568453\n",
            "Data columns (total 10 columns):\n",
            " #   Column                  Non-Null Count   Dtype \n",
            "---  ------                  --------------   ----- \n",
            " 0   Id                      568454 non-null  int64 \n",
            " 1   ProductId               568454 non-null  object\n",
            " 2   UserId                  568454 non-null  object\n",
            " 3   ProfileName             568438 non-null  object\n",
            " 4   HelpfulnessNumerator    568454 non-null  int64 \n",
            " 5   HelpfulnessDenominator  568454 non-null  int64 \n",
            " 6   Score                   568454 non-null  int64 \n",
            " 7   Time                    568454 non-null  int64 \n",
            " 8   Summary                 568427 non-null  object\n",
            " 9   Text                    568454 non-null  object\n",
            "dtypes: int64(5), object(5)\n",
            "memory usage: 43.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "rRiXs88lWRrI",
        "outputId": "2089dce0-a1dc-43cb-adc2-9819736399b3"
      },
      "source": [
        "reviews.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  ...                                               Text\n",
              "0   1  ...  I have bought several of the Vitality canned d...\n",
              "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2   3  ...  This is a confection that has been around a fe...\n",
              "3   4  ...  If you are looking for the secret ingredient i...\n",
              "4   5  ...  Great taffy at a great price.  There was a wid...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZI1osqZ1nkn",
        "outputId": "eaf58de1-17bc-44a9-8dab-05484f1bf4ad"
      },
      "source": [
        "type(reviews['Score'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsKJdCWy1u7Y"
      },
      "source": [
        "text = []\n",
        "score = []\n",
        "for i, row in reviews.iterrows():\n",
        "  text.append(row['Text'])\n",
        "  score.append(str(row['Score']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-wFMeQF3-aj",
        "outputId": "114ca245-5b73-43d4-b7b1-c5a2ac8d4dbd"
      },
      "source": [
        "type(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "W_4MigdN4kjv",
        "outputId": "ad473192-eb21-4238-c281-42795d3ae726"
      },
      "source": [
        "data = {'Text':reviews['Text'],'Score':score}\n",
        "\n",
        "text_data = pd.DataFrame(data ,columns=['Text','Score'] )\n",
        "\n",
        "text_data.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text Score\n",
              "0  I have bought several of the Vitality canned d...     5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xokNn7qZqVAz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "26449975-58c7-4944-e1b2-7df1673e14b5"
      },
      "source": [
        "data = {'Text':reviews['Text'].astype(str) ,'Score':reviews['Score'].astype(int)}\n",
        "\n",
        "text_data = pd.DataFrame(data ,columns=['Text','Score'] )\n",
        "\n",
        "text_data.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Score\n",
              "0  I have bought several of the Vitality canned d...      5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaobN-yGeBND",
        "outputId": "b20825a7-1024-4ce9-b472-fc68dbd06d66"
      },
      "source": [
        "#Droping nan datapoints from DataFrame\n",
        "text_data = text_data.dropna()\n",
        "\n",
        "# check if we have any nan values are there\n",
        "print(text_data.isnull().values.any())\n",
        "print(\"number of nan values\",text_data.isnull().values.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "number of nan values 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GZt7pVkqVA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e9f307b-bb92-45ff-dad6-3c29a9db6a52"
      },
      "source": [
        "reviews = text_data[text_data['Score'] != 3]\n",
        "\n",
        "#text_data = reviews['Text']\n",
        "#score_data = reviews['Score']\n",
        "#if score> 3, set score = 1\n",
        "#if score<=2, set score = 0\n",
        "#if score == 3, remove the rows.\n",
        "reviews.shape\n",
        "#text_data.Score.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(525814, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "Ss0YzOaFyAdW",
        "outputId": "1d600a71-09f7-4830-f029-385c36c63493"
      },
      "source": [
        "d= reviews['Score'].apply(lambda x: 1 if x > 3 else 0)\n",
        "reviews['Score'] = d\n",
        "reviews.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Score\n",
              "0  I have bought several of the Vitality canned d...      1\n",
              "1  Product arrived labeled as Jumbo Salted Peanut...      0\n",
              "2  This is a confection that has been around a fe...      1\n",
              "3  If you are looking for the secret ingredient i...      0\n",
              "4  Great taffy at a great price.  There was a wid...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdmKFV1pmRJv",
        "outputId": "630925da-d40e-4f8b-c2b4-0c9b0ddaa2e4"
      },
      "source": [
        "reviews.Score.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    443777\n",
              "0     82037\n",
              "Name: Score, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYZ-UB9UqVA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e17775-9ec3-4904-9e8e-3911132bfcf1"
      },
      "source": [
        "from random import sample\n",
        "def get_wordlen(x):\n",
        "    return len(x.split())\n",
        "reviews['len'] = reviews.Text.apply(get_wordlen)\n",
        "reviews = reviews[reviews.len<50]\n",
        "\n",
        "reviews = reviews.sample(n=100000, random_state=30)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj-UPWnMngOl",
        "outputId": "dd861aab-d90c-429d-dc98-0df8f3ccba29"
      },
      "source": [
        "reviews.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmxPPL7NrqfT",
        "outputId": "497d3857-0443-4309-b2b0-5d33576b6292"
      },
      "source": [
        "text_data = reviews['Text']\n",
        "print(text_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64117     The tea was of great quality and it tasted lik...\n",
            "418112    My cat loves this.  The pellets are nice and s...\n",
            "357829    Great product. Does not completely get rid of ...\n",
            "175872    This gum is my favorite!  I would advise every...\n",
            "178716    I also found out about this product because of...\n",
            "                                ...                        \n",
            "336657    Using this coffee and a stove top espresso mak...\n",
            "498034    THE TASTE OF THIS M&M IS THE BEST. I USED IT I...\n",
            "357766    Excellent Tea. I enjoy a cup every now and the...\n",
            "326811    These oatmeal cookies have a great spice taste...\n",
            "19261     This is the best coffee ever! I will never dri...\n",
            "Name: Text, Length: 100000, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch9mYs8ask8q",
        "outputId": "8ce552d3-c9b7-410a-df21-5fef74db7c88"
      },
      "source": [
        "#https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string\n",
        "import re\n",
        "\n",
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('<.*?>')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  return cleantext\n",
        "\n",
        "reviews['Text'] = [cleanhtml(i) for i in reviews['Text']]\n",
        "reviews['Text'].head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64117     The tea was of great quality and it tasted lik...\n",
              "418112    My cat loves this.  The pellets are nice and s...\n",
              "357829    Great product. Does not completely get rid of ...\n",
              "175872    This gum is my favorite!  I would advise every...\n",
              "178716    I also found out about this product because of...\n",
              "Name: Text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvldQriGqVBB"
      },
      "source": [
        "#remove HTML from the Text column and save in the Text column only"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhfN1s2mqVBD"
      },
      "source": [
        "#print head 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGz4kGOFwYh6"
      },
      "source": [
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test  = train_test_split(reviews['Text'], reviews['Score'], test_size=0.2, stratify=reviews['Score'],random_state = 33)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsYDd3okqVBF"
      },
      "source": [
        "#split the data into train and test data(20%) with Stratify sampling, random state 33,"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "4bdaC_s2Ak6S",
        "outputId": "eab1178e-4e3e-4bfb-b665-1a471acb65f5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "train_class_distribution = y_train.value_counts().sort_index()\n",
        "test_class_distribution = y_test.value_counts().sort_index()\n",
        "\n",
        "my_colors = ['r', 'g', 'b', 'k', 'y', 'm', 'c']\n",
        "train_class_distribution.plot(kind='bar',color=my_colors)\n",
        "plt.xlabel('score')\n",
        "plt.ylabel('Data points per score')\n",
        "plt.title('Distribution of score  in train data')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "GMfvUHhPBjTn",
        "outputId": "80d22a00-c3df-4691-b473-6b6c685c2434"
      },
      "source": [
        "my_colors = ['r', 'g', 'b', 'k', 'y', 'm', 'c']\n",
        "test_class_distribution.plot(kind='bar',color=my_colors)\n",
        "plt.xlabel('score')\n",
        "plt.ylabel('Data points per score')\n",
        "plt.title('Distribution of score  in test data')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q6OAcrOqVBI"
      },
      "source": [
        "#plot bar graphs of y_train and y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up-z5boWqVBK"
      },
      "source": [
        "#saving to disk. if we need, we can load preprocessed data directly.\n",
        "reviews.to_csv('preprocessed.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBtqNGN9qVBM"
      },
      "source": [
        "<pre><font size=6>Part-2: Creating BERT Model</font>\n",
        "\n",
        "If you want to know more about BERT, You can watch live sessions on Transformers and BERt.\n",
        "we will strongly recommend you to read <a href=\"https://jalammar.github.io/illustrated-transformer/\">Transformers</a>, <a href=\"https://arxiv.org/abs/1810.04805\">BERT Paper</a> and, <a href=\"https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\">This blog</a>.\n",
        "\n",
        "\n",
        "For this assignment, we are using <a href=\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\">BERT uncased Base model</a>.\n",
        "It uses L=12 hidden layers (i.e., Transformer blocks), a hidden size of H=768, and A=12 attention heads. </pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8xd2HejqVBN"
      },
      "source": [
        "## Loading the Pretrained Model from tensorflow HUB\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# maximum length of a seq in the data we have, for now i am making it as 55. You can change this\n",
        "max_seq_length = 55\n",
        "\n",
        "#BERT takes 3 inputs\n",
        "\n",
        "#this is input words. Sequence of words represented as integers\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "\n",
        "#mask vector if you are padding anything\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
        "\n",
        "#segment vectors. If you are giving only one sentence for the classification, total seg vector is 0.\n",
        "#If you are giving two sentenced with [sep] token separated, first seq segment vectors are zeros and\n",
        "#second seq segment vector are 1's\n",
        "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "#bert layer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "#Bert model\n",
        "#We are using only pooled output not sequence out.\n",
        "#If you want to know about those, please read https://www.kaggle.com/questions-and-answers/86510\n",
        "bert_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=pooled_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQJsjg6fqVBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e805796f-64a1-4b35-de13-ee7bcde51841"
      },
      "source": [
        "bert_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 55)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 55)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 55)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 109,482,241\n",
            "Trainable params: 0\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3z0OMA5qVBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264c0429-b1fe-484a-dcaa-9ef669f8f76e"
      },
      "source": [
        "bert_model.output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'keras_layer/StatefulPartitionedCall:0' shape=(None, 768) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewv4hFCsqVBU"
      },
      "source": [
        "<pre><font size=6>Part-3: Tokenization</font></pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX3VEFjiqVBU"
      },
      "source": [
        "#getting Vocab file\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07gzvr7qK66W",
        "outputId": "347e66f2-baec-49f4-998d-559a7e5f712e"
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Tokenization classes implementation.\n",
        "\n",
        "The file is forked from:\n",
        "https://github.com/google-research/bert/blob/master/tokenization.py.\n",
        "\"\"\"\n",
        "!pip install sentencepiece\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "import six\n",
        "import tensorflow as tf\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "SPIECE_UNDERLINE = \"\"\n",
        "\n",
        "\n",
        "def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n",
        "  \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n",
        "\n",
        "  # The casing has to be passed in by the user and there is no explicit check\n",
        "  # as to whether it matches the checkpoint. The casing information probably\n",
        "  # should have been stored in the bert_config.json file, but it's not, so\n",
        "  # we have to heuristically detect it to validate.\n",
        "\n",
        "  if not init_checkpoint:\n",
        "    return\n",
        "\n",
        "  m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n",
        "  if m is None:\n",
        "    return\n",
        "\n",
        "  model_name = m.group(1)\n",
        "\n",
        "  lower_models = [\n",
        "      \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n",
        "      \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  cased_models = [\n",
        "      \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n",
        "      \"multi_cased_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  is_bad_config = False\n",
        "  if model_name in lower_models and not do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"False\"\n",
        "    case_name = \"lowercased\"\n",
        "    opposite_flag = \"True\"\n",
        "\n",
        "  if model_name in cased_models and do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"True\"\n",
        "    case_name = \"cased\"\n",
        "    opposite_flag = \"False\"\n",
        "\n",
        "  if is_bad_config:\n",
        "    raise ValueError(\n",
        "        \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n",
        "        \"However, `%s` seems to be a %s model, so you \"\n",
        "        \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n",
        "        \"how the model was pre-training. If this error is wrong, please \"\n",
        "        \"just comment out this check.\" %\n",
        "        (actual_flag, init_checkpoint, model_name, case_name, opposite_flag))\n",
        "\n",
        "\n",
        "def convert_to_unicode(text):\n",
        "  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    elif isinstance(text, unicode):\n",
        "      return text\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def printable_text(text):\n",
        "  \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
        "\n",
        "  # These functions want `str` for both Python2 and Python3, but in one case\n",
        "  # it's a Unicode string and in the other it's a byte string.\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, unicode):\n",
        "      return text.encode(\"utf-8\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "  vocab = collections.OrderedDict()\n",
        "  index = 0\n",
        "  with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n",
        "    while True:\n",
        "      token = convert_to_unicode(reader.readline())\n",
        "      if not token:\n",
        "        break\n",
        "      token = token.strip()\n",
        "      vocab[token] = index\n",
        "      index += 1\n",
        "  return vocab\n",
        "\n",
        "\n",
        "def convert_by_vocab(vocab, items):\n",
        "  \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
        "  output = []\n",
        "  for item in items:\n",
        "    output.append(vocab[item])\n",
        "  return output\n",
        "\n",
        "\n",
        "def convert_tokens_to_ids(vocab, tokens):\n",
        "  return convert_by_vocab(vocab, tokens)\n",
        "\n",
        "\n",
        "def convert_ids_to_tokens(inv_vocab, ids):\n",
        "  return convert_by_vocab(inv_vocab, ids)\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "  text = text.strip()\n",
        "  if not text:\n",
        "    return []\n",
        "  tokens = text.split()\n",
        "  return tokens\n",
        "\n",
        "\n",
        "class FullTokenizer(object):\n",
        "  \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_file, do_lower_case=True, split_on_punc=True):\n",
        "    self.vocab = load_vocab(vocab_file)\n",
        "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "    self.basic_tokenizer = BasicTokenizer(\n",
        "        do_lower_case=do_lower_case, split_on_punc=split_on_punc)\n",
        "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    split_tokens = []\n",
        "    for token in self.basic_tokenizer.tokenize(text):\n",
        "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "        split_tokens.append(sub_token)\n",
        "\n",
        "    return split_tokens\n",
        "\n",
        "  def convert_tokens_to_ids(self, tokens):\n",
        "    return convert_by_vocab(self.vocab, tokens)\n",
        "\n",
        "  def convert_ids_to_tokens(self, ids):\n",
        "    return convert_by_vocab(self.inv_vocab, ids)\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "  \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "  def __init__(self, do_lower_case=True, split_on_punc=True):\n",
        "    \"\"\"Constructs a BasicTokenizer.\n",
        "\n",
        "    Args:\n",
        "      do_lower_case: Whether to lower case the input.\n",
        "      split_on_punc: Whether to apply split on punctuations. By default BERT\n",
        "        starts a new token for punctuations. This makes detokenization difficult\n",
        "        for tasks like seq2seq decoding.\n",
        "    \"\"\"\n",
        "    self.do_lower_case = do_lower_case\n",
        "    self.split_on_punc = split_on_punc\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "    text = convert_to_unicode(text)\n",
        "    text = self._clean_text(text)\n",
        "\n",
        "    # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "    # models. This is also applied to the English models now, but it doesn't\n",
        "    # matter since the English models were not trained on any Chinese data\n",
        "    # and generally don't have any Chinese data in them (there are Chinese\n",
        "    # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "    # words in the English Wikipedia.).\n",
        "    text = self._tokenize_chinese_chars(text)\n",
        "\n",
        "    orig_tokens = whitespace_tokenize(text)\n",
        "    split_tokens = []\n",
        "    for token in orig_tokens:\n",
        "      if self.do_lower_case:\n",
        "        token = token.lower()\n",
        "        token = self._run_strip_accents(token)\n",
        "      if self.split_on_punc:\n",
        "        split_tokens.extend(self._run_split_on_punc(token))\n",
        "      else:\n",
        "        split_tokens.append(token)\n",
        "\n",
        "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "    return output_tokens\n",
        "\n",
        "  def _run_strip_accents(self, text):\n",
        "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "    text = unicodedata.normalize(\"NFD\", text)\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cat = unicodedata.category(char)\n",
        "      if cat == \"Mn\":\n",
        "        continue\n",
        "      output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _run_split_on_punc(self, text):\n",
        "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "    chars = list(text)\n",
        "    i = 0\n",
        "    start_new_word = True\n",
        "    output = []\n",
        "    while i < len(chars):\n",
        "      char = chars[i]\n",
        "      if _is_punctuation(char):\n",
        "        output.append([char])\n",
        "        start_new_word = True\n",
        "      else:\n",
        "        if start_new_word:\n",
        "          output.append([])\n",
        "        start_new_word = False\n",
        "        output[-1].append(char)\n",
        "      i += 1\n",
        "\n",
        "    return [\"\".join(x) for x in output]\n",
        "\n",
        "  def _tokenize_chinese_chars(self, text):\n",
        "    \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if self._is_chinese_char(cp):\n",
        "        output.append(\" \")\n",
        "        output.append(char)\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _is_chinese_char(self, cp):\n",
        "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "    #\n",
        "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "    # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "    # space-separated words, so they are not treated specially and handled\n",
        "    # like the all of the other languages.\n",
        "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "  def _clean_text(self, text):\n",
        "    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "        continue\n",
        "      if _is_whitespace(char):\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "  \"\"\"Runs WordPiece tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
        "    self.vocab = vocab\n",
        "    self.unk_token = unk_token\n",
        "    self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "\n",
        "    This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "    using the given vocabulary.\n",
        "\n",
        "    For example:\n",
        "      input = \"unaffable\"\n",
        "      output = [\"un\", \"##aff\", \"##able\"]\n",
        "\n",
        "    Args:\n",
        "      text: A single token or whitespace separated tokens. This should have\n",
        "        already been passed through `BasicTokenizer.\n",
        "\n",
        "    Returns:\n",
        "      A list of wordpiece tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    text = convert_to_unicode(text)\n",
        "\n",
        "    output_tokens = []\n",
        "    for token in whitespace_tokenize(text):\n",
        "      chars = list(token)\n",
        "      if len(chars) > self.max_input_chars_per_word:\n",
        "        output_tokens.append(self.unk_token)\n",
        "        continue\n",
        "\n",
        "      is_bad = False\n",
        "      start = 0\n",
        "      sub_tokens = []\n",
        "      while start < len(chars):\n",
        "        end = len(chars)\n",
        "        cur_substr = None\n",
        "        while start < end:\n",
        "          substr = \"\".join(chars[start:end])\n",
        "          if start > 0:\n",
        "            substr = \"##\" + substr\n",
        "          if substr in self.vocab:\n",
        "            cur_substr = substr\n",
        "            break\n",
        "          end -= 1\n",
        "        if cur_substr is None:\n",
        "          is_bad = True\n",
        "          break\n",
        "        sub_tokens.append(cur_substr)\n",
        "        start = end\n",
        "\n",
        "      if is_bad:\n",
        "        output_tokens.append(self.unk_token)\n",
        "      else:\n",
        "        output_tokens.extend(sub_tokens)\n",
        "    return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "  \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "  # \\t, \\n, and \\r are technically control characters but we treat them\n",
        "  # as whitespace since they are generally considered as such.\n",
        "  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat == \"Zs\":\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "  \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "  # These are technically control characters but we count them as whitespace\n",
        "  # characters.\n",
        "  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return False\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat in (\"Cc\", \"Cf\"):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "  \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "  cp = ord(char)\n",
        "  # We treat all non-letter/number ASCII as punctuation.\n",
        "  # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "  # Punctuation class but we treat them as punctuation anyways, for\n",
        "  # consistency.\n",
        "  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat.startswith(\"P\"):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def preprocess_text(inputs, remove_space=True, lower=False):\n",
        "  \"\"\"Preprocesses data by removing extra space and normalize data.\n",
        "\n",
        "  This method is used together with sentence piece tokenizer and is forked from:\n",
        "  https://github.com/google-research/google-research/blob/master/albert/tokenization.py\n",
        "\n",
        "  Args:\n",
        "    inputs: The input text.\n",
        "    remove_space: Whether to remove the extra space.\n",
        "    lower: Whether to lowercase the text.\n",
        "\n",
        "  Returns:\n",
        "    The preprocessed text.\n",
        "\n",
        "  \"\"\"\n",
        "  outputs = inputs\n",
        "  if remove_space:\n",
        "    outputs = \" \".join(inputs.strip().split())\n",
        "\n",
        "  if six.PY2 and isinstance(outputs, str):\n",
        "    try:\n",
        "      outputs = six.ensure_text(outputs, \"utf-8\")\n",
        "    except UnicodeDecodeError:\n",
        "      outputs = six.ensure_text(outputs, \"latin-1\")\n",
        "\n",
        "  outputs = unicodedata.normalize(\"NFKD\", outputs)\n",
        "  outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
        "  if lower:\n",
        "    outputs = outputs.lower()\n",
        "\n",
        "  return outputs\n",
        "\n",
        "\n",
        "def encode_pieces(sp_model, text, sample=False):\n",
        "  \"\"\"Segements text into pieces.\n",
        "\n",
        "  This method is used together with sentence piece tokenizer and is forked from:\n",
        "  https://github.com/google-research/google-research/blob/master/albert/tokenization.py\n",
        "\n",
        "\n",
        "  Args:\n",
        "    sp_model: A spm.SentencePieceProcessor object.\n",
        "    text: The input text to be segemented.\n",
        "    sample: Whether to randomly sample a segmentation output or return a\n",
        "      deterministic one.\n",
        "\n",
        "  Returns:\n",
        "    A list of token pieces.\n",
        "  \"\"\"\n",
        "  if six.PY2 and isinstance(text, six.text_type):\n",
        "    text = six.ensure_binary(text, \"utf-8\")\n",
        "\n",
        "  if not sample:\n",
        "    pieces = sp_model.EncodeAsPieces(text)\n",
        "  else:\n",
        "    pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
        "  new_pieces = []\n",
        "  for piece in pieces:\n",
        "    piece = printable_text(piece)\n",
        "    if len(piece) > 1 and piece[-1] == \",\" and piece[-2].isdigit():\n",
        "      cur_pieces = sp_model.EncodeAsPieces(piece[:-1].replace(\n",
        "          SPIECE_UNDERLINE, \"\"))\n",
        "      if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "        if len(cur_pieces[0]) == 1:\n",
        "          cur_pieces = cur_pieces[1:]\n",
        "        else:\n",
        "          cur_pieces[0] = cur_pieces[0][1:]\n",
        "      cur_pieces.append(piece[-1])\n",
        "      new_pieces.extend(cur_pieces)\n",
        "    else:\n",
        "      new_pieces.append(piece)\n",
        "\n",
        "  return new_pieces\n",
        "\n",
        "\n",
        "def encode_ids(sp_model, text, sample=False):\n",
        "  \"\"\"Segments text and return token ids.\n",
        "\n",
        "  This method is used together with sentence piece tokenizer and is forked from:\n",
        "  https://github.com/google-research/google-research/blob/master/albert/tokenization.py\n",
        "\n",
        "  Args:\n",
        "    sp_model: A spm.SentencePieceProcessor object.\n",
        "    text: The input text to be segemented.\n",
        "    sample: Whether to randomly sample a segmentation output or return a\n",
        "      deterministic one.\n",
        "\n",
        "  Returns:\n",
        "    A list of token ids.\n",
        "  \"\"\"\n",
        "  pieces = encode_pieces(sp_model, text, sample=sample)\n",
        "  ids = [sp_model.PieceToId(piece) for piece in pieces]\n",
        "  return ids\n",
        "\n",
        "\n",
        "class FullSentencePieceTokenizer(object):\n",
        "  \"\"\"Runs end-to-end sentence piece tokenization.\n",
        "\n",
        "  The interface of this class is intended to keep the same as above\n",
        "  `FullTokenizer` class for easier usage.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, sp_model_file):\n",
        "    \"\"\"Inits FullSentencePieceTokenizer.\n",
        "\n",
        "    Args:\n",
        "      sp_model_file: The path to the sentence piece model file.\n",
        "    \"\"\"\n",
        "    self.sp_model = spm.SentencePieceProcessor()\n",
        "    self.sp_model.Load(sp_model_file)\n",
        "    self.vocab = {\n",
        "        self.sp_model.IdToPiece(i): i\n",
        "        for i in six.moves.range(self.sp_model.GetPieceSize())\n",
        "    }\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes text into pieces.\"\"\"\n",
        "    return encode_pieces(self.sp_model, text)\n",
        "\n",
        "  def convert_tokens_to_ids(self, tokens):\n",
        "    \"\"\"Converts a list of tokens to a list of ids.\"\"\"\n",
        "    return [self.sp_model.PieceToId(printable_text(token)) for token in tokens]\n",
        "\n",
        "  def convert_ids_to_tokens(self, ids):\n",
        "    \"\"\"Converts a list of ids ot a list of tokens.\"\"\"\n",
        "    return [self.sp_model.IdToPiece(id_) for id_ in ids]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |                               | 10kB 13.9MB/s eta 0:00:01\r\u001b[K     |                               | 20kB 18.5MB/s eta 0:00:01\r\u001b[K     |                               | 30kB 13.1MB/s eta 0:00:01\r\u001b[K     |                              | 40kB 10.6MB/s eta 0:00:01\r\u001b[K     |                              | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |                              | 61kB 6.0MB/s eta 0:00:01\r\u001b[K     |                              | 71kB 5.7MB/s eta 0:00:01\r\u001b[K     |                             | 81kB 6.3MB/s eta 0:00:01\r\u001b[K     |                             | 92kB 6.6MB/s eta 0:00:01\r\u001b[K     |                             | 102kB 5.3MB/s eta 0:00:01\r\u001b[K     |                            | 112kB 5.3MB/s eta 0:00:01\r\u001b[K     |                            | 122kB 5.3MB/s eta 0:00:01\r\u001b[K     |                            | 133kB 5.3MB/s eta 0:00:01\r\u001b[K     |                           | 143kB 5.3MB/s eta 0:00:01\r\u001b[K     |                           | 153kB 5.3MB/s eta 0:00:01\r\u001b[K     |                           | 163kB 5.3MB/s eta 0:00:01\r\u001b[K     |                           | 174kB 5.3MB/s eta 0:00:01\r\u001b[K     |                          | 184kB 5.3MB/s eta 0:00:01\r\u001b[K     |                          | 194kB 5.3MB/s eta 0:00:01\r\u001b[K     |                          | 204kB 5.3MB/s eta 0:00:01\r\u001b[K     |                         | 215kB 5.3MB/s eta 0:00:01\r\u001b[K     |                         | 225kB 5.3MB/s eta 0:00:01\r\u001b[K     |                         | 235kB 5.3MB/s eta 0:00:01\r\u001b[K     |                         | 245kB 5.3MB/s eta 0:00:01\r\u001b[K     |                        | 256kB 5.3MB/s eta 0:00:01\r\u001b[K     |                        | 266kB 5.3MB/s eta 0:00:01\r\u001b[K     |                        | 276kB 5.3MB/s eta 0:00:01\r\u001b[K     |                       | 286kB 5.3MB/s eta 0:00:01\r\u001b[K     |                       | 296kB 5.3MB/s eta 0:00:01\r\u001b[K     |                       | 307kB 5.3MB/s eta 0:00:01\r\u001b[K     |                       | 317kB 5.3MB/s eta 0:00:01\r\u001b[K     |                      | 327kB 5.3MB/s eta 0:00:01\r\u001b[K     |                      | 337kB 5.3MB/s eta 0:00:01\r\u001b[K     |                      | 348kB 5.3MB/s eta 0:00:01\r\u001b[K     |                     | 358kB 5.3MB/s eta 0:00:01\r\u001b[K     |                     | 368kB 5.3MB/s eta 0:00:01\r\u001b[K     |                     | 378kB 5.3MB/s eta 0:00:01\r\u001b[K     |                    | 389kB 5.3MB/s eta 0:00:01\r\u001b[K     |                    | 399kB 5.3MB/s eta 0:00:01\r\u001b[K     |                    | 409kB 5.3MB/s eta 0:00:01\r\u001b[K     |                    | 419kB 5.3MB/s eta 0:00:01\r\u001b[K     |                   | 430kB 5.3MB/s eta 0:00:01\r\u001b[K     |                   | 440kB 5.3MB/s eta 0:00:01\r\u001b[K     |                   | 450kB 5.3MB/s eta 0:00:01\r\u001b[K     |                  | 460kB 5.3MB/s eta 0:00:01\r\u001b[K     |                  | 471kB 5.3MB/s eta 0:00:01\r\u001b[K     |                  | 481kB 5.3MB/s eta 0:00:01\r\u001b[K     |                  | 491kB 5.3MB/s eta 0:00:01\r\u001b[K     |                 | 501kB 5.3MB/s eta 0:00:01\r\u001b[K     |                 | 512kB 5.3MB/s eta 0:00:01\r\u001b[K     |                 | 522kB 5.3MB/s eta 0:00:01\r\u001b[K     |                | 532kB 5.3MB/s eta 0:00:01\r\u001b[K     |                | 542kB 5.3MB/s eta 0:00:01\r\u001b[K     |                | 552kB 5.3MB/s eta 0:00:01\r\u001b[K     |               | 563kB 5.3MB/s eta 0:00:01\r\u001b[K     |               | 573kB 5.3MB/s eta 0:00:01\r\u001b[K     |               | 583kB 5.3MB/s eta 0:00:01\r\u001b[K     |               | 593kB 5.3MB/s eta 0:00:01\r\u001b[K     |              | 604kB 5.3MB/s eta 0:00:01\r\u001b[K     |              | 614kB 5.3MB/s eta 0:00:01\r\u001b[K     |              | 624kB 5.3MB/s eta 0:00:01\r\u001b[K     |             | 634kB 5.3MB/s eta 0:00:01\r\u001b[K     |             | 645kB 5.3MB/s eta 0:00:01\r\u001b[K     |             | 655kB 5.3MB/s eta 0:00:01\r\u001b[K     |             | 665kB 5.3MB/s eta 0:00:01\r\u001b[K     |            | 675kB 5.3MB/s eta 0:00:01\r\u001b[K     |            | 686kB 5.3MB/s eta 0:00:01\r\u001b[K     |            | 696kB 5.3MB/s eta 0:00:01\r\u001b[K     |           | 706kB 5.3MB/s eta 0:00:01\r\u001b[K     |           | 716kB 5.3MB/s eta 0:00:01\r\u001b[K     |           | 727kB 5.3MB/s eta 0:00:01\r\u001b[K     |          | 737kB 5.3MB/s eta 0:00:01\r\u001b[K     |          | 747kB 5.3MB/s eta 0:00:01\r\u001b[K     |          | 757kB 5.3MB/s eta 0:00:01\r\u001b[K     |          | 768kB 5.3MB/s eta 0:00:01\r\u001b[K     |         | 778kB 5.3MB/s eta 0:00:01\r\u001b[K     |         | 788kB 5.3MB/s eta 0:00:01\r\u001b[K     |         | 798kB 5.3MB/s eta 0:00:01\r\u001b[K     |        | 808kB 5.3MB/s eta 0:00:01\r\u001b[K     |        | 819kB 5.3MB/s eta 0:00:01\r\u001b[K     |        | 829kB 5.3MB/s eta 0:00:01\r\u001b[K     |        | 839kB 5.3MB/s eta 0:00:01\r\u001b[K     |       | 849kB 5.3MB/s eta 0:00:01\r\u001b[K     |       | 860kB 5.3MB/s eta 0:00:01\r\u001b[K     |       | 870kB 5.3MB/s eta 0:00:01\r\u001b[K     |      | 880kB 5.3MB/s eta 0:00:01\r\u001b[K     |      | 890kB 5.3MB/s eta 0:00:01\r\u001b[K     |      | 901kB 5.3MB/s eta 0:00:01\r\u001b[K     |     | 911kB 5.3MB/s eta 0:00:01\r\u001b[K     |     | 921kB 5.3MB/s eta 0:00:01\r\u001b[K     |     | 931kB 5.3MB/s eta 0:00:01\r\u001b[K     |     | 942kB 5.3MB/s eta 0:00:01\r\u001b[K     |    | 952kB 5.3MB/s eta 0:00:01\r\u001b[K     |    | 962kB 5.3MB/s eta 0:00:01\r\u001b[K     |    | 972kB 5.3MB/s eta 0:00:01\r\u001b[K     |   | 983kB 5.3MB/s eta 0:00:01\r\u001b[K     |   | 993kB 5.3MB/s eta 0:00:01\r\u001b[K     |   | 1.0MB 5.3MB/s eta 0:00:01\r\u001b[K     |   | 1.0MB 5.3MB/s eta 0:00:01\r\u001b[K     |  | 1.0MB 5.3MB/s eta 0:00:01\r\u001b[K     |  | 1.0MB 5.3MB/s eta 0:00:01\r\u001b[K     |  | 1.0MB 5.3MB/s eta 0:00:01\r\u001b[K     | | 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     | | 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     | | 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     || 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     || 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     || 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     || 1.1MB 5.3MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoIEFP3hJm0C"
      },
      "source": [
        "tokenizer = FullTokenizer(vocab_file, do_lower_case )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_iPwa99qVBW"
      },
      "source": [
        "#import tokenization - We have given tokenization.py file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guJMLJ8bqVBY"
      },
      "source": [
        "# Create tokenizer \" Instantiate FullTokenizer\"\n",
        "# name must be \"tokenizer\"\n",
        "# the FullTokenizer takes two parameters 1. vocab_file and 2. do_lower_case\n",
        "# we have created these in the above cell ex: FullTokenizer(vocab_file, do_lower_case )\n",
        "# please check the \"tokenization.py\" file the complete implementation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9crhPylQqVBg"
      },
      "source": [
        "# Create train and test tokens (X_train_tokens, X_test_tokens) from (X_train, X_test) using Tokenizer and\n",
        "\n",
        "# add '[CLS]' at start of the Tokens and '[SEP]' at the end of the tokens.\n",
        "\n",
        "# maximum number of tokens is 55(We already given this to BERT layer above) so shape is (None, 55)\n",
        "\n",
        "# if it is less than 55, add '[PAD]' token else truncate the tokens length.(similar to padding)\n",
        "\n",
        "# Based on padding, create the mask for Train and Test ( 1 for real token, 0 for '[PAD]'),\n",
        "# it will also same shape as input tokens (None, 55) save those in X_train_mask, X_test_mask\n",
        "\n",
        "# Create a segment input for train and test. We are using only one sentence so all zeros. This shape will also (None, 55)\n",
        "\n",
        "# type of all the above arrays should be numpy arrays\n",
        "\n",
        "# after execution of this cell, you have to get\n",
        "# X_train_tokens, X_train_mask, X_train_segment\n",
        "# X_test_tokens, X_test_mask, X_test_segment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhIDatlCOHdD",
        "outputId": "b2b94a6c-43f9-4c9e-f6f2-5be0269072c8"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "print('original sentence :\\n' , np.array(X_train.values[0].split()))\n",
        "print('number of words :',len(X_train.values[0].split()))\n",
        "print('='*50)\n",
        "max_seq_length = 55\n",
        "tokens = tokenizer.tokenize(X_train.values[0])\n",
        "tokens = tokens[0:(max_seq_length-2)]\n",
        "tokens = ['[CLS]',*tokens,'[SEP]']\n",
        "print('tokens are \"\\n', np.array(tokens))\n",
        "print('='*50)\n",
        "print('number of tokens :',len(tokens))\n",
        "print('token replace with positional encoding :\\n',np.array(tokenizer.convert_tokens_to_ids(tokens)))\n",
        "print('='*50)\n",
        "print('the mask array is ',np.array([1]*len(tokens) + [0]*(max_seq_length-len(tokens))))\n",
        "print('='*50)\n",
        "print('the sequence length is ',np.array([0]*max_seq_length))\n",
        "print('='*50)\n",
        "#X_train_encoded_essay = t.texts_to_sequences(X_train['essay'])\n",
        "y = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
        "z = [int(i) for i in y]\n",
        "\n",
        "x = []\n",
        "for i in range(max_seq_length):\n",
        "  if i < len(z):\n",
        "    x.append(int(z[i]))\n",
        "  else:\n",
        "    x.append(int(0))\n",
        "\n",
        "print(x)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original sentence :\n",
            " ['I' 'had' 'never' 'tried' 'this' 'brand' 'before,' 'so' 'I' 'was'\n",
            " 'worried' 'about' 'the' 'quality.' 'It' 'tasted' 'great.' 'A' 'very'\n",
            " 'nice' 'smooth' 'rich' 'full' 'flavor.' 'Its' 'my' 'new' 'favoret.']\n",
            "number of words : 28\n",
            "==================================================\n",
            "tokens are \"\n",
            " ['[CLS]' 'i' 'had' 'never' 'tried' 'this' 'brand' 'before' ',' 'so' 'i'\n",
            " 'was' 'worried' 'about' 'the' 'quality' '.' 'it' 'tasted' 'great' '.' 'a'\n",
            " 'very' 'nice' 'smooth' 'rich' 'full' 'flavor' '.' 'its' 'my' 'new'\n",
            " 'favor' '##et' '.' '[SEP]']\n",
            "==================================================\n",
            "number of tokens : 36\n",
            "token replace with positional encoding :\n",
            " [  101  1045  2018  2196  2699  2023  4435  2077  1010  2061  1045  2001\n",
            "  5191  2055  1996  3737  1012  2009 12595  2307  1012  1037  2200  3835\n",
            "  5744  4138  2440 14894  1012  2049  2026  2047  5684  3388  1012   102]\n",
            "==================================================\n",
            "the mask array is  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "==================================================\n",
            "the sequence length is  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "==================================================\n",
            "[101, 1045, 2018, 2196, 2699, 2023, 4435, 2077, 1010, 2061, 1045, 2001, 5191, 2055, 1996, 3737, 1012, 2009, 12595, 2307, 1012, 1037, 2200, 3835, 5744, 4138, 2440, 14894, 1012, 2049, 2026, 2047, 5684, 3388, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv1-t4OjqVBj"
      },
      "source": [
        "#### Example\n",
        "<img src='https://i.imgur.com/5AhhmgU.png'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5vIY4PWZJgs"
      },
      "source": [
        "max_seq_length = 55\n",
        "def sen_to_token(data):\n",
        "  Tokens , mask , sequence = list(), list(), list()\n",
        "  for i in range(len(data)):\n",
        "    tokens = tokenizer.tokenize(X_train.values[i])\n",
        "    tokens = tokens[0:(max_seq_length-2)]\n",
        "    tokens = ['[CLS]',*tokens,'[SEP]']\n",
        "    tokenizer.convert_tokens_to_ids(tokens)\n",
        "    y = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
        "    z = [int(i) for i in y]\n",
        "    x = []\n",
        "    for i in range(max_seq_length):\n",
        "      if i < len(z):\n",
        "        x.append(int(z[i]))\n",
        "      else:\n",
        "        x.append(int(0))\n",
        "    Tokens.append(np.array(x))\n",
        "\n",
        "    mask.append(np.array([1]*len(tokens) + [0]*(max_seq_length-len(tokens))))\n",
        "    sequence.append(np.array([0]*max_seq_length))\n",
        "\n",
        "  return  np.array(Tokens) , np.array(mask) , np.array(sequence)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw7GbqFMb_p8"
      },
      "source": [
        "X_train_tokens, X_train_mask, X_train_segment = sen_to_token(X_train)\n",
        "X_test_tokens, X_test_mask, X_test_segment = sen_to_token(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpsytUtjfKOr"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF0idMRDqVBm"
      },
      "source": [
        "\n",
        "##save all your results to disk so that, no need to run all again.\n",
        "pickle.dump((X_train, X_train_tokens, X_train_mask, X_train_segment, y_train),open('train_data.pkl','wb'))\n",
        "pickle.dump((X_test, X_test_tokens, X_test_mask, X_test_segment, y_test),open('test_data.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Leu1URGzqVBo"
      },
      "source": [
        "#you can load from disk\n",
        "X_train, X_train_tokens, X_train_mask, X_train_segment, y_train = pickle.load(open(\"train_data.pkl\", 'rb'))\n",
        "X_test, X_test_tokens, X_test_mask, X_test_segment, y_test = pickle.load(open(\"test_data.pkl\", 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qekHJgmdqVBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261d6af9-b98f-44cc-fe9f-cd17dfe56a71"
      },
      "source": [
        "def grader_alltokens_train():\n",
        "    out = False\n",
        "\n",
        "    if type(X_train_tokens) == np.ndarray:\n",
        "\n",
        "        temp_shapes = (X_train_tokens.shape[1]==max_seq_length) and (X_train_mask.shape[1]==max_seq_length) and \\\n",
        "        (X_train_segment.shape[1]==max_seq_length)\n",
        "\n",
        "        segment_temp = not np.any(X_train_segment)\n",
        "\n",
        "        mask_temp = np.sum(X_train_mask==0) == np.sum(X_train_tokens==0)\n",
        "\n",
        "        no_cls = np.sum(X_train_tokens==tokenizer.vocab['[CLS]'])==X_train_tokens.shape[0]\n",
        "\n",
        "        no_sep = np.sum(X_train_tokens==tokenizer.vocab['[SEP]'])==X_train_tokens.shape[0]\n",
        "\n",
        "        out = temp_shapes and segment_temp and mask_temp and no_cls and no_sep\n",
        "\n",
        "    else:\n",
        "        print('Type of all above token arrays should be numpy array not list')\n",
        "        out = False\n",
        "    assert(out==True)\n",
        "    return out\n",
        "\n",
        "grader_alltokens_train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av4SRMPSqVBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba985d35-b239-4444-862b-f7a339106dfb"
      },
      "source": [
        "def grader_alltokens_test():\n",
        "    out = False\n",
        "    if type(X_test_tokens) == np.ndarray:\n",
        "\n",
        "        temp_shapes = (X_test_tokens.shape[1]==max_seq_length) and (X_test_mask.shape[1]==max_seq_length) and \\\n",
        "        (X_test_segment.shape[1]==max_seq_length)\n",
        "\n",
        "        segment_temp = not np.any(X_test_segment)\n",
        "\n",
        "        mask_temp = np.sum(X_test_mask==0) == np.sum(X_test_tokens==0)\n",
        "\n",
        "        no_cls = np.sum(X_test_tokens==tokenizer.vocab['[CLS]'])==X_test_tokens.shape[0]\n",
        "\n",
        "        no_sep = np.sum(X_test_tokens==tokenizer.vocab['[SEP]'])==X_test_tokens.shape[0]\n",
        "\n",
        "        out = temp_shapes and segment_temp and mask_temp and no_cls and no_sep\n",
        "\n",
        "    else:\n",
        "        print('Type of all above token arrays should be numpy array not list')\n",
        "        out = False\n",
        "    assert(out==True)\n",
        "    return out\n",
        "grader_alltokens_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEj-Eua5qVBx"
      },
      "source": [
        "<pre><font size=6>Part-4: Getting Embeddings from BERT Model</font>\n",
        "We already created the BERT model in the part-2 and input data in the part-3.\n",
        "We will utlize those two and will get the embeddings for each sentence in the\n",
        "Train and test data.</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwOVgQFDqVBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3883558-f227-4602-f033-490fccc45165"
      },
      "source": [
        "bert_model.input"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'input_word_ids:0' shape=(None, 55) dtype=int32>,\n",
              " <tf.Tensor 'input_mask:0' shape=(None, 55) dtype=int32>,\n",
              " <tf.Tensor 'segment_ids:0' shape=(None, 55) dtype=int32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcpkQq1OqVB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b89ecd-b580-4a40-f071-69c6fe3da981"
      },
      "source": [
        "bert_model.output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'keras_layer/StatefulPartitionedCall:0' shape=(None, 768) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxdIlOIBlm7j"
      },
      "source": [
        "# get the train output, BERT model will give one output so save in\n",
        "# X_train_pooled_output\n",
        "X_train_pooled_output=bert_model.predict([X_train_tokens[:100],X_train_mask[:100],X_train_segment[:100]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZT11BCol4gL"
      },
      "source": [
        "# get the test output, BERT model will give one output so save in\n",
        "# X_test_pooled_output\n",
        "X_test_pooled_output=bert_model.predict([X_test_tokens[:100],X_test_mask[:100],X_test_segment[:100]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL6JVojfqVB8"
      },
      "source": [
        "##save all your results to disk so that, no need to run all again.\n",
        "pickle.dump((X_train_pooled_output, X_test_pooled_output),open('final_output.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHCsW0IvqVCB"
      },
      "source": [
        "#now we have X_train_pooled_output, y_train\n",
        "#X_test_pooled_ouput, y_test\n",
        "\n",
        "#please use this grader to evaluate\n",
        "def greader_output():\n",
        "    assert(X_train_pooled_output.shape[1]==768)\n",
        "    assert(len(y_train)==len(X_train_pooled_output))\n",
        "    assert(X_test_pooled_output.shape[1]==768)\n",
        "    assert(len(y_test)==len(X_test_pooled_output))\n",
        "    assert(len(y_train.shape)==1)\n",
        "    assert(len(X_train_pooled_output.shape)==2)\n",
        "    assert(len(y_test.shape)==1)\n",
        "    assert(len(X_test_pooled_output.shape)==2)\n",
        "    return True\n",
        "greader_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYwS1QbAqVCD"
      },
      "source": [
        "<pre><font size=6>Part-5: Training a NN with 768 features</font>\n",
        "\n",
        "Create a NN and train the NN.\n",
        "1.<b> You have to use AUC as metric.</b>\n",
        "2. You can use any architecture you want.\n",
        "3. You have to use tensorboard to log all your metrics and Losses. You have to send those logs.\n",
        "4. Print the loss and metric at every epoch.\n",
        "5. You have to submit without overfitting and underfitting.\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od8PQlYRqVCE"
      },
      "source": [
        "##imports\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import os\n",
        "import random as rn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSnmX3WnqVCG"
      },
      "source": [
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "##https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
        "## Have to clear the session. If you are not clearing, Graph will create again and again and graph size will increses.\n",
        "## Varibles will also set to some value from before session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "## Set the random seed values to regenerate the model.\n",
        "np.random.seed(0)\n",
        "rn.seed(0)\n",
        "\n",
        "inputs_essay = Input(shape=(768,), dtype='int32')\n",
        "\n",
        "dense1 = Dense(500, activation='relu')(inputs_essay)\n",
        "\n",
        "drop_out1 = Dropout(0.1)(dense1)\n",
        "\n",
        "dense2 = Dense(20, activation='relu')(drop_out1)\n",
        "\n",
        "drop_out2 = Dropout(0.1)(dense2)\n",
        "\n",
        "dense3 = Dense(5, activation='relu')(drop_out2)\n",
        "\n",
        "Out = Dense(units=2,activation='softmax')(dense3)\n",
        "\n",
        "#Creating a model\n",
        "model = Model(inputs_essay,outputs=Out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOMtiC6GHAY2"
      },
      "source": [
        "#compiling\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyYxTW2UHfXs"
      },
      "source": [
        "Y_train = tf.keras.utils.to_categorical(y_train, 2)[:100]\n",
        "Y_test = tf.keras.utils.to_categorical(y_test, 2)[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QexCKfQJHI6G",
        "outputId": "9e57c3f9-a532-4853-e8cd-0386dbfe9d9e"
      },
      "source": [
        "model.fit(X_train_pooled_output,Y_train,epochs=2,validation_data=(X_test_pooled_output ,Y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 0.5531 - accuracy: 0.7700 - val_loss: 0.3504 - val_accuracy: 0.8900\n",
            "Epoch 2/2\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4244 - accuracy: 0.8800 - val_loss: 0.3378 - val_accuracy: 0.8900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f01882f36d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcILeYZI9pxm"
      },
      "source": [
        "<Pre><font size=6>Part-6: Creating a Data pipeline for BERT Model</font>\n",
        "\n",
        "1. Download data from <a href=\"https://drive.google.com/file/d/1QwjqTsqTX2vdy7fTmeXjxP3dq8IAVLpo/view?usp=sharing\">here</a>\n",
        "2. Read the csv file\n",
        "3. Remove all the html tags\n",
        "4. Now do tokenization [Part 3 as mentioned above]\n",
        "    * Create tokens,mask array and segment array\n",
        "5. Get Embeddings from BERT Model [Part 4 as mentioned above] , let it be X_test\n",
        "   * Print the shape of output(X_test.shape).You should get (352,768)\n",
        "6. Predit the output of X_test with the Neural network model which we trained earlier.\n",
        "7. Print the occurences of class labels in the predicted output\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiBeOnna9yDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51bbfcb3-0d9f-47f8-fa81-b8f5da94493c"
      },
      "source": [
        "#Read the dataset - Amazon fine food reviews\n",
        "reviews = pd.read_csv(\"test.csv\")\n",
        "#reviews.to_frame()\n",
        "#check the info of the dataset\n",
        "reviews.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 352 entries, 0 to 351\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Text    352 non-null    object\n",
            "dtypes: object(1)\n",
            "memory usage: 2.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcTJxyVzKPZj",
        "outputId": "cf0f33f0-7bc2-4943-e872-6f6a8f2defb4"
      },
      "source": [
        "#https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string\n",
        "import re\n",
        "\n",
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('<.*?>')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  return cleantext\n",
        "\n",
        "reviews['Text'] = [cleanhtml(i) for i in reviews['Text']]\n",
        "reviews['Text'].head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Just opened Greenies Joint Care (individually ...\n",
              "1    This product rocks :) My mom was very happy w/...\n",
              "2    The product was fine, but the cost of shipping...\n",
              "3    I love this soup. It's great as part of a meal...\n",
              "4    Getting ready to order again. These are great ...\n",
              "Name: Text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpxPiyrYKzrJ"
      },
      "source": [
        "X_train_tokens, X_train_mask, X_train_segment = sen_to_token(reviews['Text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6eeOyL7LrE1"
      },
      "source": [
        "X_train_pooled_output=bert_model.predict([X_train_tokens[:100],X_train_mask[:100],X_train_segment[:100]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4PEQSooL3Bx"
      },
      "source": [
        "y_pred = model.predict(X_train_pooled_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSTjj7YiMcJM",
        "outputId": "6f356ffa-61f2-442a-9266-ac1ccbbbc69d"
      },
      "source": [
        "y_pred[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.10036853, 0.89963144], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7v8ex6oMe5D"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}